{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment of Analysis of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Pre-Processing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                           date             user  sentiment     query  \n",
       "0  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
       "1  Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
       "2  Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
       "3  Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
       "4  Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/sentiment140.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Label Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    800000\n",
      "4    800000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MENTION URL  Awww thats a bummer  You shoulda ...</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MENTION I dived many times for the ball Manage...</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENTION no its not behaving at all im mad why ...</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  MENTION URL  Awww thats a bummer  You shoulda ...   \n",
       "1  is upset that he cant update his Facebook by t...   \n",
       "2  MENTION I dived many times for the ball Manage...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  MENTION no its not behaving at all im mad why ...   \n",
       "\n",
       "                           date             user  sentiment     query  \n",
       "0  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
       "1  Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
       "2  Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
       "3  Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
       "4  Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+', 'URL', text)\n",
    "    text = re.sub(r'@\\w+', 'MENTION', text)\n",
    "    text = re.sub(r'#\\w+', 'HASHTAG', text)\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[MENTION, URL, Awww, thats, a, bummer, You, sh...</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[is, upset, that, he, cant, update, his, Faceb...</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[MENTION, I, dived, many, time, for, the, ball...</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[my, whole, body, feel, itchy, and, like, it, ...</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MENTION, no, it, not, behaving, at, all, im, ...</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [MENTION, URL, Awww, thats, a, bummer, You, sh...   \n",
       "1  [is, upset, that, he, cant, update, his, Faceb...   \n",
       "2  [MENTION, I, dived, many, time, for, the, ball...   \n",
       "3  [my, whole, body, feel, itchy, and, like, it, ...   \n",
       "4  [MENTION, no, it, not, behaving, at, all, im, ...   \n",
       "\n",
       "                           date             user  sentiment     query  \n",
       "0  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
       "1  Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
       "2  Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
       "3  Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
       "4  Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "df['text'] = df['text'].apply(tokenize_and_lemmatize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "print(set(df['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print first 20 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [MENTION, URL, Awww, thats, a, bummer, You, sh...\n",
      "1     [is, upset, that, he, cant, update, his, Faceb...\n",
      "2     [MENTION, I, dived, many, time, for, the, ball...\n",
      "3     [my, whole, body, feel, itchy, and, like, it, ...\n",
      "4     [MENTION, no, it, not, behaving, at, all, im, ...\n",
      "5                      [MENTION, not, the, whole, crew]\n",
      "6                                        [Need, a, hug]\n",
      "7     [MENTION, hey, long, time, no, see, Yes, Rains...\n",
      "8                [MENTION, nope, they, didnt, have, it]\n",
      "9                             [MENTION, que, me, muera]\n",
      "10        [spring, break, in, plain, city, it, snowing]\n",
      "11                        [I, just, repierced, my, ear]\n",
      "12    [MENTION, I, couldnt, bear, to, watch, it, And...\n",
      "13    [MENTION, It, it, count, idk, why, I, did, eit...\n",
      "14    [MENTION, i, wouldve, been, the, first, but, i...\n",
      "15    [MENTION, I, wish, I, got, to, watch, it, with...\n",
      "16    [Hollis, death, scene, will, hurt, me, severel...\n",
      "17                               [about, to, file, tax]\n",
      "18    [MENTION, ahh, ive, always, wanted, to, see, r...\n",
      "19    [MENTION, Oh, dear, Were, you, drinking, out, ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['text'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Vectorization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max([len(seq) for seq in X_train_seq])\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "embedding_dim = 300\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model:\n",
    "        embedding_matrix[i] = w2v_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 58, 300)           113484300 \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               219648    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113704077 (433.75 MB)\n",
      "Trainable params: 219777 (858.50 KB)\n",
      "Non-trainable params: 113484300 (432.91 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2500/2500 [==============================] - 459s 183ms/step - loss: 0.4625 - accuracy: 0.7802 - val_loss: 0.4291 - val_accuracy: 0.8015\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 459s 183ms/step - loss: 0.4184 - accuracy: 0.8075 - val_loss: 0.4075 - val_accuracy: 0.8127\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 461s 184ms/step - loss: 0.4017 - accuracy: 0.8165 - val_loss: 0.4031 - val_accuracy: 0.8178\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 462s 185ms/step - loss: 0.3903 - accuracy: 0.8227 - val_loss: 0.3941 - val_accuracy: 0.8224\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 459s 184ms/step - loss: 0.3813 - accuracy: 0.8277 - val_loss: 0.3860 - val_accuracy: 0.8252\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded, y_train, epochs=5, batch_size=512, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 98s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.83      0.82    159494\n",
      "    Positive       0.83      0.83      0.83    160506\n",
      "\n",
      "    accuracy                           0.83    320000\n",
      "   macro avg       0.83      0.83      0.83    320000\n",
      "weighted avg       0.83      0.83      0.83    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(\"int64\")\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'], zero_division=1)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
