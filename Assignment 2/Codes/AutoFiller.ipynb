{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Filler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams, LaplaceProbDist, SimpleGoodTuringProbDist\n",
    "from hazm import sent_tokenize, word_tokenize, Normalizer, stopwords_list\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>پراید ستون جدید</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...\n",
       "1  چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...\n",
       "2                                    پراید ستون جدید\n",
       "3  اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...\n",
       "4  گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv(r'Datasets\\digikala_comment.csv')\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def preprocess_sentences(comments_df):\n",
    "    # Convert comments into sentences\n",
    "    normalizer = Normalizer()\n",
    "    comments_df['sentences'] = comments_df['comment'].apply(lambda comment: sent_tokenize(comment))\n",
    "    comments_df.drop('comment', axis=1, inplace=True)\n",
    "\n",
    "    # Convert DataFrame into a list of sentences\n",
    "    sentences_list = comments_df['sentences'].tolist()\n",
    "    flat_sentences_list = [sentence for sublist in sentences_list for sentence in sublist]\n",
    "\n",
    "    normalized_sentences = [normalizer.normalize(s) for s in flat_sentences_list]\n",
    "\n",
    "    # Remove Zero-Width Non-Joiners\n",
    "    sentences = [sentence.replace('\\u200c', ' ') for sentence in normalized_sentences]\n",
    "\n",
    "    # Remove English characters from sentences\n",
    "    cleaned_sentences = []\n",
    "    english_pattern = re.compile(r'[a-zA-Z]')\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = ' '.join(word for word in word_tokenize(sentence) if not english_pattern.search(word))\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    # Remove punctuations from sentences\n",
    "    punctuations = string.punctuation + '،' + '؟'\n",
    "    cleaned_sentences_no_punc = []\n",
    "    for sentence in cleaned_sentences:\n",
    "        cleaned_sentence = ''.join(char for char in sentence if char not in punctuations)\n",
    "        cleaned_sentences_no_punc.append(cleaned_sentence)\n",
    "\n",
    "    # Remove stop words (Top unigrams are stop words) - stop_words() from hazm reduces ability to predict, only top stop words are removed\n",
    "    # stopwords = stopwords_list()\n",
    "    stopwords = ['و', 'از', 'این', 'که', 'به', 'هم', 'خیلی', 'رو', 'با', 'در', 'برای']\n",
    "    cleaned_sentences_no_stopwords = []\n",
    "    for sentence in cleaned_sentences_no_punc:\n",
    "        cleaned_sentence = ' '.join(word for word in word_tokenize(sentence) if word not in stopwords)\n",
    "        cleaned_sentences_no_stopwords.append(cleaned_sentence)\n",
    "\n",
    "    # Remove emojis\n",
    "    cleaned_sentences_no_emojis = [remove_emoji(sentence) for sentence in cleaned_sentences_no_stopwords]\n",
    "\n",
    "    return cleaned_sentences_no_emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = preprocess_sentences(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sententences: 463\n",
      "نسبت قیمتش ارزش خرید داره جاداره طراحیش قشنگه تنها مشکلش بندهای ضعیفش هست باعث میشه استحکام چندانی نداشنه باشه\n",
      "چند ماهی میشه گرفتمش\n",
      "برنامه نویسی کارای گرافیکی ازش استفاده می کنم\n",
      "واقعا هر لحاظ بگین عالیه\n",
      "پراید ستون جدید\n",
      "اقا همه چیش خوبه فقط پایین زیاد حاشیه داره روشن شدن گوشی بیشتر میشه\n",
      "نکته دیگه اینکه خاطر اطرافش یه کوچلو خمیده هست گلس بعد یه مدتی جدا مشیه\n",
      "ولی کل قیمت بهترین گوشی هست همه چی داره دوربین گرفته تا رم سی پی یو گرافیک حسگر های مختلف چیزای دیگه\n",
      "گوسی هو اوی ۱۰ سیپیو دوربین رمش بهتره خودتون میتونین برین تمام مقایسه های ۱۰ گوشیو ببینین\n",
      "چادر سبک زیباییه دوختشم عالیه\n"
     ]
    }
   ],
   "source": [
    "print('Number of sententences:', len(c))\n",
    "for i in range(10):\n",
    "    print(c[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(sentences):\n",
    "    # Tokenize sentences into words\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    unigrams = [word for sentence in tokenized_sentences for word in sentence]\n",
    "    bigrams_list = list(bigrams(unigrams))\n",
    "    trigrams_list = list(trigrams(unigrams))\n",
    "    \n",
    "    return unigrams, bigrams_list, trigrams_list\n",
    "\n",
    "def count_ngrams(unigrams, bigrams_list, trigrams_list):\n",
    "    unigram_counts = Counter(unigrams)\n",
    "    bigram_counts = Counter(bigrams_list)\n",
    "    trigram_counts = Counter(trigrams_list)\n",
    "    \n",
    "    return unigram_counts, bigram_counts, trigram_counts\n",
    "\n",
    "def report_most_frequent_ngrams(unigram_counts, bigram_counts, trigram_counts):\n",
    "    print(\"Most frequent unigrams:\")\n",
    "    for word, count in unigram_counts.most_common(8):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    print(\"\\nMost frequent bigrams:\")\n",
    "    for bigram, count in bigram_counts.most_common(8):\n",
    "        print(f\"{' '.join(bigram)}: {count}\")\n",
    "\n",
    "    print(\"\\nMost frequent trigrams:\")\n",
    "    for trigram, count in trigram_counts.most_common(8):\n",
    "        print(f\"{' '.join(trigram)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent unigrams:\n",
      "من: 102\n",
      "می: 98\n",
      "داره: 80\n",
      "گوشی: 72\n",
      "ولی: 64\n",
      "هست: 58\n",
      "کیفیت: 51\n",
      "خوب: 50\n",
      "\n",
      "Most frequent bigrams:\n",
      "می کنم: 28\n",
      "پیشنهاد می: 15\n",
      "دیجی کالا: 15\n",
      "خوبی داره: 12\n",
      "ممنون دیجی: 11\n",
      "فوق العاده: 11\n",
      "نسبت قیمتش: 10\n",
      "راضی هستم: 10\n",
      "\n",
      "Most frequent trigrams:\n",
      "پیشنهاد می کنم: 15\n",
      "حتما پیشنهاد می: 8\n",
      "می کنم ممنون: 6\n",
      "استفاده می کنم: 5\n",
      "ممنون دیجی کالا: 5\n",
      "کنم ممنون دیجی: 5\n",
      "خریدم راضی هستم: 4\n",
      "تو شگفت انگیز: 4\n"
     ]
    }
   ],
   "source": [
    "unigrams_list, bigrams_list, trigrams_list = extract_ngrams(c)\n",
    "uc, bc, tc = count_ngrams(unigrams_list, bigrams_list, trigrams_list)\n",
    "report_most_frequent_ngrams(uc, bc, tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing in N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Smoothing Techniques in N-grams Language Models\n",
    "\n",
    "In n-grams language models, it's common to encounter unseen n-grams, i.e., sequences of words that never occurred in the training data. When calculating the probability of such unseen n-grams, their probability would be zero, which could lead to severe issues in the model's performance, especially during evaluation.\n",
    "\n",
    "To address this problem, smoothing techniques are used. Smoothing assigns a small non-zero probability to unseen n-grams, thereby preventing zero probabilities and making the model more robust. Smoothing techniques distribute the probability mass from observed n-grams to unseen ones in a principled manner.\n",
    "\n",
    "---\n",
    "\n",
    "**Laplace (Add-One) Smoothing**\n",
    "\n",
    "Laplace smoothing, also known as Add-One smoothing, is one of the simplest smoothing techniques. In Laplace smoothing, a count of 1 is added to each observed n-gram count before calculating probabilities. This ensures that no n-gram has zero probability and prevents unseen n-grams from having zero probabilities.\n",
    "\n",
    "Mathematically, the formula for Laplace smoothing of an n-gram is:\n",
    "\n",
    "$ P_{\\text{Laplace}}(w_n | w_{n-1}) = \\frac{{\\text{count}(w_{n-1}w_n) + 1}}{{\\text{count}(w_{n-1}) + V}}$\n",
    "\n",
    "Where:\n",
    "- $\\text{count}(w_{n-1}w_n)$ is the count of the n-gram \\( $w_{n-1}w_n$ \\) in the training data.\n",
    "- $\\text{count}(w_{n-1}) $ is the count of the preceding (n-1)-gram \\( $w_{n-1} $\\) in the training data.\n",
    "- $ V $ is the vocabulary size, representing the total number of unique words in the training data.\n",
    "\n",
    "https://www.nltk.org/api/nltk.probability.LaplaceProbDist.html\n",
    "\n",
    "---\n",
    "\n",
    "**Good-Turing Smoothing**\n",
    "\n",
    "Good-Turing smoothing is a more sophisticated smoothing technique that estimates the probabilities of unseen n-grams based on the observed frequencies of other n-grams. It adjusts the probabilities of unseen n-grams based on the frequencies of seen n-grams with similar frequencies. This technique tends to work well when dealing with sparse data and can provide more accurate estimates than Laplace smoothing.\n",
    "\n",
    "Good-Turing smoothing uses a statistical method called the Good-Turing frequency estimation to estimate the probability of unseen n-grams. It estimates the probability of an unseen n-gram by considering the frequency of n-grams with similar counts in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(unigrams, bigrams, trigrams):\n",
    "    unigram_prob_dist = LaplaceProbDist(nltk.FreqDist(unigrams), bins=len(set(unigrams)))\n",
    "\n",
    "    bigram_prob_dist = SimpleGoodTuringProbDist(nltk.FreqDist(bigrams))\n",
    "    trigram_prob_dist = SimpleGoodTuringProbDist(nltk.FreqDist(trigrams))\n",
    "\n",
    "    return unigram_prob_dist, bigram_prob_dist, trigram_prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_prob_dist, bigram_prob_dist, trigram_prob_dist = calculate_probabilities(unigrams_list, bigrams_list, trigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002027780594139714 1.3032889040272414e-05 0.00016514349283038886\n"
     ]
    }
   ],
   "source": [
    "unigram_prob = unigram_prob_dist.prob('بو')\n",
    "bigram_prob = bigram_prob_dist.prob(('جاداره', 'طراحیش'))\n",
    "trigram_prob = trigram_prob_dist.prob(('ارزش', 'خرید', 'داره'))\n",
    "\n",
    "print(unigram_prob, bigram_prob, trigram_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(tokens, freq_dist):\n",
    "    N = len(tokens)\n",
    "    log_prob_sum = sum(np.log(freq_dist.prob(ngram)) for ngram in tokens)\n",
    "    perplexity = -log_prob_sum / N\n",
    "    return perplexity\n",
    "\n",
    "def calculate_perplexity_for_models(unigram_prob_dist, bigram_prob_dist, trigram_prob_dist, sentences):\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    perplexities = {'Unigram': [], 'Bigram': [], 'Trigram': []}\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        unigram_perplexity = calculate_perplexity(list(sentence), unigram_prob_dist)\n",
    "        bigram_perplexity = calculate_perplexity(list(bigrams(sentence)), bigram_prob_dist)\n",
    "        trigram_perplexity = calculate_perplexity(list(trigrams(sentence)), trigram_prob_dist)\n",
    "        \n",
    "        perplexities['Unigram'].append(unigram_perplexity)\n",
    "        perplexities['Bigram'].append(bigram_perplexity)\n",
    "        perplexities['Trigram'].append(trigram_perplexity)\n",
    "    \n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram Model:\n",
      "Sentence 1: 7.950880401943151\n",
      "Sentence 2: 7.417571047865991\n",
      "Sentence 3: 7.097150839971258\n",
      "Sentence 4: 7.030403772914723\n",
      "-------------------------------\n",
      "Perplexity for Bigram Model:\n",
      "Sentence 1: 4.392854897224514\n",
      "Sentence 2: 6.215513579141406\n",
      "Sentence 3: 6.324829342155599\n",
      "Sentence 4: 4.676729809176047\n",
      "-------------------------------\n",
      "Perplexity for Trigram Model:\n",
      "Sentence 1: 1.8766268890431816\n",
      "Sentence 2: 4.340036784899916\n",
      "Sentence 3: 3.2622949554625955\n",
      "Sentence 4: 3.724184310935733\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"این لپ تاپ سخت افزار خیلی قوی داره و از پس هرکاری به راحتی بر میاد\",\n",
    "    \"این ساعت بسیار زیبا طراحی و ساخته شده\",\n",
    "    \"یک محصول با کیفیت ایرانی که حقیقتا جای حمایت داره\",\n",
    "    \"بوش و ماندگاری خوب هست من خیلی دوستش دارم\"\n",
    "]\n",
    "\n",
    "# preprocessed_test_sentences = preprocess_sentences(pd.DataFrame({'comment': test_sentences}))\n",
    "perplexities = calculate_perplexity_for_models(unigram_prob_dist, bigram_prob_dist, trigram_prob_dist, test_sentences)\n",
    "\n",
    "for model, perplexity_list in perplexities.items():\n",
    "    print(f\"Perplexity for {model} Model:\")\n",
    "    for sentence_index, perplexity in enumerate(perplexity_list):\n",
    "        print(f\"Sentence {sentence_index + 1}: {perplexity}\")\n",
    "    print('-------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_predict(sent, freq_dist, max_length=12):\n",
    "    while len(sent.split()) < max_length:\n",
    "        next_word = freq_dist.generate()\n",
    "        sent = sent + \" \" + next_word\n",
    "    return sent\n",
    "\n",
    "\n",
    "def bigram_predict(sent, freq_dist, max_length=12):\n",
    "    while len(sent.split()) < max_length:\n",
    "        last_word = sent.split()[-1]\n",
    "        candidates = [sample for sample in freq_dist.samples() if sample[0] == last_word]\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            print(\"2-gram not found in model\")\n",
    "            return\n",
    "\n",
    "        next_word = max(candidates, key=lambda x: freq_dist.prob(x))\n",
    "        sent = sent + \" \" + next_word[1]\n",
    "    return sent\n",
    "    \n",
    "\n",
    "def trigram_predict(sent, freq_dist, max_length=12):\n",
    "    while len(sent.split()) < max_length:\n",
    "        last_word_1 = sent.split()[-1]\n",
    "        last_word_2 = sent.split()[-2]\n",
    "        candidates = [sample for sample in freq_dist.samples() if sample[1] == last_word_1 and sample[0] == last_word_2]\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            print(\"3-gram not found in model\")\n",
    "            return\n",
    "\n",
    "        next_word = max(candidates, key=lambda x: freq_dist.prob(x))\n",
    "        sent = sent + \" \" + next_word[2]\n",
    "    return sent\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_sentences = [\n",
    "    \"کیفیت محصولات چینی زرین\",\n",
    "    \"از لحاظ جنس جنس خوبی داره\",\n",
    "    \"حتما پیشنهاد می کنم\",\n",
    "    \"بعد از چند روز استفاده\"\n",
    "]\n",
    "\n",
    "unigram_predicted_sentences = []\n",
    "bigram_predicted_sentences = []\n",
    "trigram_predicted_sentences = []\n",
    "\n",
    "for sent in test_sentences:\n",
    "    sentence = unigram_predict(sent, unigram_prob_dist)\n",
    "    unigram_predicted_sentences.append(sentence)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    sentence = bigram_predict(sent, bigram_prob_dist)\n",
    "    bigram_predicted_sentences.append(sentence)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    sentence = trigram_predict(sent, trigram_prob_dist)\n",
    "    trigram_predicted_sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Original Sentence: کیفیت محصولات چینی زرین\n",
      "Unigram Prediction: کیفیت محصولات چینی زرین خوشبوعه رسما کلا روی باحاله صدای نفس یکسال\n",
      "Bigram Prediction: کیفیت محصولات چینی زرین عالیه من گوشی را بزنید ۴ سال هست\n",
      "Trigram Prediction: کیفیت محصولات چینی زرین عالیه قیمتش عالی نسبت بقیه خردکن ها زیباتره\n",
      "-------------------------------------------------------------\n",
      "Original Sentence: از لحاظ جنس جنس خوبی داره\n",
      "Unigram Prediction: از لحاظ جنس جنس خوبی داره خشک کشاورزی تا های من کردم\n",
      "Bigram Prediction: از لحاظ جنس جنس خوبی داره ولی اگه موقع شستن حواستون نباشه\n",
      "Trigram Prediction: از لحاظ جنس جنس خوبی داره طراحی شیشه ی قشنگی داره هدیه\n",
      "-------------------------------------------------------------\n",
      "Original Sentence: حتما پیشنهاد می کنم\n",
      "Unigram Prediction: حتما پیشنهاد می کنم شیک طولانی همه سازگار ادم خانم داشتیم های\n",
      "Bigram Prediction: حتما پیشنهاد می کنم ممنون دیجی کالا خریدم راضی هستم بسیار عالی\n",
      "Trigram Prediction: حتما پیشنهاد می کنم ممنون دیجی کالا خریدم پنج تاش راضیم اول\n",
      "-------------------------------------------------------------\n",
      "Original Sentence: بعد از چند روز استفاده\n",
      "Unigram Prediction: بعد از چند روز استفاده صورتم داره چهار من هر دوست داشت\n",
      "Bigram Prediction: بعد از چند روز استفاده می کنم ممنون دیجی کالا خریدم راضی\n",
      "Trigram Prediction: بعد از چند روز استفاده کمر درد شدم فقط یکم گرونه ولی\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Original Sentence:', test_sentences[i])\n",
    "    print('Unigram Prediction:', unigram_predicted_sentences[i])\n",
    "    print('Bigram Prediction:', bigram_predicted_sentences[i])\n",
    "    print('Trigram Prediction:', trigram_predicted_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity for Generated Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: کیفیت محصولات چینی زرین خوشبوعه رسما کلا روی باحاله صدای نفس یکسال\n",
      "Unigram Perplexity: 7.718349427451994\n",
      "Bigram Perplexity: 4.198899855475713\n",
      "Trigram Perplexity: 2.615649857800202\n",
      "Sentence: از لحاظ جنس جنس خوبی داره خشک کشاورزی تا های من کردم\n",
      "Unigram Perplexity: 6.3924021905274175\n",
      "Bigram Perplexity: 3.601116995327521\n",
      "Trigram Perplexity: 3.56004791584508\n",
      "Sentence: حتما پیشنهاد می کنم شیک طولانی همه سازگار ادم خانم داشتیم های\n",
      "Unigram Perplexity: 6.898916332759886\n",
      "Bigram Perplexity: 1.8774217774298114\n",
      "Trigram Perplexity: 1.3862065336005838\n",
      "Sentence: بعد از چند روز استفاده صورتم داره چهار من هر دوست داشت\n",
      "Unigram Perplexity: 6.7110038433918975\n",
      "Bigram Perplexity: 1.9404913597849038\n",
      "Trigram Perplexity: 1.3223596624754177\n"
     ]
    }
   ],
   "source": [
    "def calculate_sentence_perplexity_unigram(sentence, freqDist):\n",
    "    tokenized_sentences = word_tokenize(sentence)\n",
    "    unigrams_list = tokenized_sentences\n",
    "    return calculate_perplexity(unigrams_list, freqDist)\n",
    "    \n",
    "\n",
    "def calculate_sentence_perplexity_bigram(sentence, freqDist):\n",
    "    tokenized_sentences = word_tokenize(sentence)\n",
    "    bigrams_list = list(bigrams(tokenized_sentences))\n",
    "    return calculate_perplexity(bigrams_list, freqDist)\n",
    "\n",
    "\n",
    "def calculate_sentence_perplexity_trigram(sentence, freqDist):\n",
    "    tokenized_sentences = word_tokenize(sentence)\n",
    "    trigrams_list = list(trigrams(tokenized_sentences))\n",
    "    return calculate_perplexity(trigrams_list, freqDist)\n",
    "\n",
    "\n",
    "\n",
    "for sentence in unigram_predicted_sentences:\n",
    "    print(f'Sentence: {sentence}')\n",
    "    print(f'Unigram Perplexity: {calculate_sentence_perplexity_unigram(sentence, unigram_prob_dist)}')\n",
    "    print(f'Bigram Perplexity: {calculate_sentence_perplexity_bigram(sentence, bigram_prob_dist)}')\n",
    "    print(f'Trigram Perplexity: {calculate_sentence_perplexity_trigram(sentence, trigram_prob_dist)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose the N in N-gram?\n",
    "\n",
    "The choice of N in N-grams, is domain dependent. \n",
    "- A google N-gram has provided helpful information for calculating relevance score -> https://www.ngrams.info/ \n",
    "- TF-IDF can be used to know the relevance of the n-gram you are extracting\n",
    "- Cross-Validation: CV can also be used in unsupervised learning -> http://arxiv.org/abs/0909.3052\n",
    "\n",
    "Choosing the right N-gram may depend on:\n",
    "1. **Computational Resources**: First thing that must be considered when choosing N for N-gram is computational and hardware resources.Training and utilizing language models with larger values of n require more computational resources in terms of memory and processing power.\n",
    "\n",
    "2. **Context Dependency**: Smaller values of n, such as unigrams or bigrams, capture local dependencies between adjacent words, whereas larger values of n, such as trigrams or higher, capture longer-range dependencies.\n",
    "\n",
    "3. **Data Size**: With larger training corpora, higher values of n can be effectively utilized to capture more complex patterns in the language. However, with limited data, using higher-order n-grams may lead to overfitting and sparse data issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
