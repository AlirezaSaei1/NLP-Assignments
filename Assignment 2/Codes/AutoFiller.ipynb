{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Filler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams, LaplaceProbDist, SimpleGoodTuringProbDist\n",
    "from hazm import sent_tokenize, word_tokenize, Normalizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>پراید ستون جدید</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...\n",
       "1  چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...\n",
       "2                                    پراید ستون جدید\n",
       "3  اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...\n",
       "4  گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv(r'Datasets\\digikala_comment.csv')\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(comments_df):\n",
    "    # Convert comments into sentences\n",
    "    normalizer = Normalizer()\n",
    "    comments_df['sentences'] = comments_df['comment'].apply(lambda comment: sent_tokenize(comment))\n",
    "    comments_df.drop('comment', axis=1, inplace=True)\n",
    "\n",
    "    # Convert DataFrame into a list of sentences\n",
    "    sentences_list = comments_df['sentences'].tolist()\n",
    "    flat_sentences_list = [sentence for sublist in sentences_list for sentence in sublist]\n",
    "\n",
    "    normalized_sentences = [normalizer.normalize(s) for s in flat_sentences_list]\n",
    "\n",
    "    # Remove Zero-Width Non-Joiners\n",
    "    sentences = [sentence.replace('\\u200c', ' ') for sentence in normalized_sentences]\n",
    "\n",
    "    # Remove English characters from sentences\n",
    "    cleaned_sentences = []\n",
    "    english_pattern = re.compile(r'[a-zA-Z]')\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = ' '.join(word for word in word_tokenize(sentence) if not english_pattern.search(word))\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    # Remove punctuations from sentences\n",
    "    punctuations = string.punctuation + '،' + '؟'\n",
    "    cleaned_sentences_final = []\n",
    "    for sentence in cleaned_sentences:\n",
    "        cleaned_sentence = ''.join(char for char in sentence if char not in punctuations)\n",
    "        cleaned_sentences_final.append(cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentences_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = preprocess_sentences(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sententences: 463\n",
      "نسبت به قیمتش ارزش خرید داره جاداره  طراحیش قشنگه تنها مشکلش بندهای ضعیفش هست که باعث میشه استحکام چندانی نداشنه باشه\n",
      "چند ماهی میشه که گرفتمش \n",
      "برای برنامه نویسی و کارای گرافیکی ازش استفاده می کنم \n",
      "واقعا از هر لحاظ بگین عالیه \n",
      "پراید ستون جدید\n",
      "اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره که با روشن شدن گوشی بیشتر هم میشه \n",
      "و نکته دیگه اینکه به خاطر این که اطرافش یه کوچلو خمیده هست گلس بعد یه مدتی جدا مشیه \n",
      "ولی در کل با این قیمت بهترین گوشی هست و همه چی داره  از دوربین گرفته تا رم و سی پی یو و گرافیک و حسگر های مختلف و خیلی چیزای دیگه \n",
      "گوسی هو اوی ۱۰ سیپیو و دوربین و رمش از این خیلی بهتره خودتون میتونین برین تمام مقایسه های ۱۰ این گوشیو ببینین\n",
      "چادر سبک و زیباییه دوختشم عالیه\n"
     ]
    }
   ],
   "source": [
    "print('Number of sententences:', len(c))\n",
    "for i in range(10):\n",
    "    print(c[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(sentences):\n",
    "    # Tokenize sentences into words\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    unigrams = [word for sentence in tokenized_sentences for word in sentence]\n",
    "    bigrams_list = list(bigrams(unigrams))\n",
    "    trigrams_list = list(trigrams(unigrams))\n",
    "    \n",
    "    return unigrams, bigrams_list, trigrams_list\n",
    "\n",
    "def count_ngrams(unigrams, bigrams_list, trigrams_list):\n",
    "    unigram_counts = Counter(unigrams)\n",
    "    bigram_counts = Counter(bigrams_list)\n",
    "    trigram_counts = Counter(trigrams_list)\n",
    "    \n",
    "    return unigram_counts, bigram_counts, trigram_counts\n",
    "\n",
    "def report_most_frequent_ngrams(unigram_counts, bigram_counts, trigram_counts):\n",
    "    print(\"Most frequent unigrams:\")\n",
    "    for word, count in unigram_counts.most_common(8):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    print(\"\\nMost frequent bigrams:\")\n",
    "    for bigram, count in bigram_counts.most_common(8):\n",
    "        print(f\"{' '.join(bigram)}: {count}\")\n",
    "\n",
    "    print(\"\\nMost frequent trigrams:\")\n",
    "    for trigram, count in trigram_counts.most_common(8):\n",
    "        print(f\"{' '.join(trigram)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent unigrams:\n",
      "و: 370\n",
      "از: 215\n",
      "که: 187\n",
      "این: 164\n",
      "به: 156\n",
      "هم: 123\n",
      "خیلی: 112\n",
      "رو: 108\n",
      "\n",
      "Most frequent bigrams:\n",
      "می کنم: 28\n",
      "نسبت به: 23\n",
      "از این: 19\n",
      "این گوشی: 18\n",
      "در کل: 17\n",
      "پیشنهاد می: 15\n",
      "دیجی کالا: 15\n",
      "بعد از: 14\n",
      "\n",
      "Most frequent trigrams:\n",
      "پیشنهاد می کنم: 15\n",
      "نسبت به قیمتش: 9\n",
      "ممنون از دیجی: 9\n",
      "حتما پیشنهاد می: 8\n",
      "از دیجی کالا: 8\n",
      "به نظر من: 7\n",
      "این گوشی رو: 6\n",
      "می کنم ممنون: 6\n"
     ]
    }
   ],
   "source": [
    "unigrams_list, bigrams_list, trigrams_list = extract_ngrams(c)\n",
    "uc, bc, tc = count_ngrams(unigrams_list, bigrams_list, trigrams_list)\n",
    "report_most_frequent_ngrams(uc, bc, tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing in N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Smoothing Techniques in N-grams Language Models\n",
    "\n",
    "In n-grams language models, it's common to encounter unseen n-grams, i.e., sequences of words that never occurred in the training data. When calculating the probability of such unseen n-grams, their probability would be zero, which could lead to severe issues in the model's performance, especially during evaluation.\n",
    "\n",
    "To address this problem, smoothing techniques are used. Smoothing assigns a small non-zero probability to unseen n-grams, thereby preventing zero probabilities and making the model more robust. Smoothing techniques distribute the probability mass from observed n-grams to unseen ones in a principled manner.\n",
    "\n",
    "---\n",
    "\n",
    "**Laplace (Add-One) Smoothing**\n",
    "\n",
    "Laplace smoothing, also known as Add-One smoothing, is one of the simplest smoothing techniques. In Laplace smoothing, a count of 1 is added to each observed n-gram count before calculating probabilities. This ensures that no n-gram has zero probability and prevents unseen n-grams from having zero probabilities.\n",
    "\n",
    "Mathematically, the formula for Laplace smoothing of an n-gram is:\n",
    "\n",
    "$ P_{\\text{Laplace}}(w_n | w_{n-1}) = \\frac{{\\text{count}(w_{n-1}w_n) + 1}}{{\\text{count}(w_{n-1}) + V}}$\n",
    "\n",
    "Where:\n",
    "- $\\text{count}(w_{n-1}w_n)$ is the count of the n-gram \\( $w_{n-1}w_n$ \\) in the training data.\n",
    "- $\\text{count}(w_{n-1}) $ is the count of the preceding (n-1)-gram \\( $w_{n-1} $\\) in the training data.\n",
    "- $ V $ is the vocabulary size, representing the total number of unique words in the training data.\n",
    "\n",
    "---\n",
    "\n",
    "**Good-Turing Smoothing**\n",
    "\n",
    "Good-Turing smoothing is a more sophisticated smoothing technique that estimates the probabilities of unseen n-grams based on the observed frequencies of other n-grams. It adjusts the probabilities of unseen n-grams based on the frequencies of seen n-grams with similar frequencies. This technique tends to work well when dealing with sparse data and can provide more accurate estimates than Laplace smoothing.\n",
    "\n",
    "Good-Turing smoothing uses a statistical method called the Good-Turing frequency estimation to estimate the probability of unseen n-grams. It estimates the probability of an unseen n-gram by considering the frequency of n-grams with similar counts in the training data.\n",
    "\n",
    "Overall, smoothing techniques like Laplace and Good-Turing are essential for handling unseen n-grams and improving the robustness of n-grams language models, especially with limited training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(unigrams, bigrams, trigrams):\n",
    "    unigram_prob_dist = LaplaceProbDist(nltk.FreqDist(unigrams), bins=len(set(unigrams)))\n",
    "\n",
    "    bigram_prob_dist = SimpleGoodTuringProbDist(nltk.FreqDist(bigrams))\n",
    "    trigram_prob_dist = SimpleGoodTuringProbDist(nltk.FreqDist(trigrams))\n",
    "\n",
    "    return unigram_prob_dist, bigram_prob_dist, trigram_prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_prob_dist, bigram_prob_dist, trigram_prob_dist = calculate_probabilities(unigrams_list, bigrams_list, trigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006305605942817656 0.002803381843181243 0.9506600302964726\n"
     ]
    }
   ],
   "source": [
    "unigram_prob = unigram_prob_dist.prob('گوشی')\n",
    "bigram_prob = bigram_prob_dist.prob(('می', 'کنم'))\n",
    "trigram_prob = trigram_prob_dist.prob(('چرا', 'چنین', 'کردی'))\n",
    "\n",
    "print(unigram_prob, bigram_prob, trigram_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(prob_dist, ngrams):\n",
    "    epsilon = 1e-10\n",
    "    log_likelihood = sum(-np.log(prob_dist.prob(ngram) + epsilon) for ngram in ngrams)\n",
    "    perplexity = np.exp(log_likelihood / len(ngrams))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_perplexity_for_models(unigram_prob_dist, bigram_prob_dist, trigram_prob_dist, sentences):\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    perplexities = {'Unigram': [], 'Bigram': [], 'Trigram': []}\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        unigram_perplexity = calculate_perplexity(unigram_prob_dist, list(sentence))\n",
    "        bigram_perplexity = calculate_perplexity(bigram_prob_dist, list(bigrams(sentence)))\n",
    "        trigram_perplexity = calculate_perplexity(trigram_prob_dist, list(trigrams(sentence)))\n",
    "        \n",
    "        perplexities['Unigram'].append(unigram_perplexity)\n",
    "        perplexities['Bigram'].append(bigram_perplexity)\n",
    "        perplexities['Trigram'].append(trigram_perplexity)\n",
    "    \n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram Model:\n",
      "Sentence 1: 648.5855924570548\n",
      "Sentence 2: 492.76504521539044\n",
      "Sentence 3: 529.1326863252393\n",
      "Sentence 4: 406.67426938699987\n",
      "-------------------------------\n",
      "Perplexity for Bigram Model:\n",
      "Sentence 1: 6727.225531108674\n",
      "Sentence 2: 26825.21801113644\n",
      "Sentence 3: 32154.60203891219\n",
      "Sentence 4: 18309.241788511776\n",
      "-------------------------------\n",
      "Perplexity for Trigram Model:\n",
      "Sentence 1: 3406.491404101158\n",
      "Sentence 2: 303733.0403873339\n",
      "Sentence 3: 303733.0403873339\n",
      "Sentence 4: 303733.0403873339\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"این لپ تاپ سخت افزار خیلی قوی داره و از پس هرکاری به راحتی بر میاد\",\n",
    "    \"این ساعت بسیار زیبا طراحی و ساخته شده\",\n",
    "    \"یک محصول با کیفیت ایرانی که حقیقتا جای حمایت داره\",\n",
    "    \"بوش و ماندگاری خوب هست من خیلی دوستش دارم\"\n",
    "]\n",
    "\n",
    "preprocessed_test_sentences = preprocess_sentences(pd.DataFrame({'comment': test_sentences}))\n",
    "perplexities = calculate_perplexity_for_models(unigram_prob_dist, bigram_prob_dist, trigram_prob_dist, preprocessed_test_sentences)\n",
    "\n",
    "for model, perplexity_list in perplexities.items():\n",
    "    print(f\"Perplexity for {model} Model:\")\n",
    "    for sentence_index, perplexity in enumerate(perplexity_list):\n",
    "        print(f\"Sentence {sentence_index + 1}: {perplexity}\")\n",
    "    print('-------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using Unigram Model:\n",
      "کیفیت محصولات چینی زرین و و و و و و و و\n",
      "از لحاظ جنس جنس خوبی داره و و و و و و\n",
      "حتما پیشنهاد میکنم و و و و و و و و و\n",
      "بعد از چند روز استفاده و و و و و و و\n",
      "------------------------\n",
      "Predictions using Bigram Model:\n",
      "کیفیت محصولات چینی زرین می می می می می می می می\n",
      "از لحاظ جنس جنس خوبی داره می می می می می می\n",
      "حتما پیشنهاد میکنم می می می می می می می می می\n",
      "بعد از چند روز استفاده می می می می می می می\n",
      "------------------------\n",
      "Predictions using Trigram Model:\n",
      "کیفیت محصولات چینی زرین پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد\n",
      "از لحاظ جنس جنس خوبی داره پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد\n",
      "حتما پیشنهاد میکنم پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد\n",
      "بعد از چند روز استفاده پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد پیشنهاد\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_next_words(model, input_sequence, max_length=12):\n",
    "    predicted_words = input_sequence.copy()\n",
    "    while len(predicted_words) < max_length:\n",
    "        if isinstance(model, SimpleGoodTuringProbDist):\n",
    "            context = predicted_words[-1:]  # Use only the last word as context\n",
    "            next_word = model.max()  # Predict the most likely word\n",
    "        elif isinstance(model, LaplaceProbDist):\n",
    "            context = predicted_words[-1:]  # Use only the last word as context for unigrams\n",
    "            next_word = model.max()  # Predict the most likely word\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported probability distribution type\")\n",
    "        \n",
    "        # Convert tuples to strings\n",
    "        if isinstance(next_word, tuple):\n",
    "            next_word = next_word[0]\n",
    "        \n",
    "        # Check if the next predicted word is the same as the last one\n",
    "        # if next_word == predicted_words[-1]:\n",
    "        #     break\n",
    "        \n",
    "        predicted_words.append(next_word)\n",
    "        if len(predicted_words) >= max_length:  # Stop if the maximum length is reached\n",
    "            break\n",
    "    return predicted_words\n",
    "\n",
    "\n",
    "# Use the function to predict the rest of the expressions for each model\n",
    "for model_name, model in {'Unigram': unigram_prob_dist, 'Bigram': bigram_prob_dist, 'Trigram': trigram_prob_dist}.items():\n",
    "    print(f\"Predictions using {model_name} Model:\")\n",
    "    for sentence in [\"کیفیت محصولات چینی زرین\", \"از لحاظ جنس جنس خوبی داره\", \"حتما پیشنهاد میکنم\", \"بعد از چند روز استفاده\"]:\n",
    "        input_sequence = word_tokenize(sentence)\n",
    "        predicted_words = predict_next_words(model, input_sequence)\n",
    "        completed_sentence = ' '.join(predicted_words)\n",
    "        print(completed_sentence)\n",
    "    print('------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
