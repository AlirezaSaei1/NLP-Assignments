{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Filler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams, LaplaceProbDist, SimpleGoodTuringProbDist\n",
    "from hazm import sent_tokenize, word_tokenize, Normalizer, stopwords_list\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>پراید ستون جدید</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...\n",
       "1  چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...\n",
       "2                                    پراید ستون جدید\n",
       "3  اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...\n",
       "4  گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv(r'Datasets\\digikala_comment.csv')\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(comments_df):\n",
    "    # Convert comments into sentences\n",
    "    normalizer = Normalizer()\n",
    "    comments_df['sentences'] = comments_df['comment'].apply(lambda comment: sent_tokenize(comment))\n",
    "    comments_df.drop('comment', axis=1, inplace=True)\n",
    "\n",
    "    # Convert DataFrame into a list of sentences\n",
    "    sentences_list = comments_df['sentences'].tolist()\n",
    "    flat_sentences_list = [sentence for sublist in sentences_list for sentence in sublist]\n",
    "\n",
    "    normalized_sentences = [normalizer.normalize(s) for s in flat_sentences_list]\n",
    "\n",
    "    # Remove Zero-Width Non-Joiners\n",
    "    sentences = [sentence.replace('\\u200c', ' ') for sentence in normalized_sentences]\n",
    "\n",
    "    # Remove English characters from sentences\n",
    "    cleaned_sentences = []\n",
    "    english_pattern = re.compile(r'[a-zA-Z]')\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = ' '.join(word for word in word_tokenize(sentence) if not english_pattern.search(word))\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    # Remove punctuations from sentences\n",
    "    punctuations = string.punctuation + '،' + '؟'\n",
    "    cleaned_sentences_no_punc = []\n",
    "    for sentence in cleaned_sentences:\n",
    "        cleaned_sentence = ''.join(char for char in sentence if char not in punctuations)\n",
    "        cleaned_sentences_no_punc.append(cleaned_sentence)\n",
    "\n",
    "    # Remove stop words (Top unigrams are stop words)\n",
    "    stopwords = stopwords_list()\n",
    "    cleaned_sentences_no_stopwords = []\n",
    "    for sentence in cleaned_sentences_no_punc:\n",
    "        cleaned_sentence = ' '.join(word for word in word_tokenize(sentence) if word not in stopwords)\n",
    "        cleaned_sentences_no_stopwords.append(cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentences_no_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = preprocess_sentences(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sententences: 463\n",
      "قیمتش ارزش خرید داره جاداره طراحیش قشنگه مشکلش بندهای ضعیفش هست میشه استحکام چندانی نداشنه باشه\n",
      "ماهی میشه گرفتمش\n",
      "برنامه نویسی کارای گرافیکی ازش استفاده\n",
      "واقعا بگین عالیه\n",
      "پراید ستون\n",
      "اقا چیش خوبه پایین حاشیه داره روشن گوشی میشه\n",
      "نکته دیگه خاطر اطرافش یه کوچلو خمیده هست گلس یه مدتی جدا مشیه\n",
      "قیمت گوشی هست چی داره دوربین رم یو گرافیک حسگر های مختلف چیزای دیگه\n",
      "گوسی هو اوی ۱۰ سیپیو دوربین رمش بهتره خودتون میتونین برین مقایسه های ۱۰ گوشیو ببینین\n",
      "چادر سبک زیباییه دوختشم عالیه\n"
     ]
    }
   ],
   "source": [
    "print('Number of sententences:', len(c))\n",
    "for i in range(10):\n",
    "    print(c[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(sentences):\n",
    "    # Tokenize sentences into words\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    unigrams = [word for sentence in tokenized_sentences for word in sentence]\n",
    "    bigrams_list = list(bigrams(unigrams))\n",
    "    trigrams_list = list(trigrams(unigrams))\n",
    "    \n",
    "    return unigrams, bigrams_list, trigrams_list\n",
    "\n",
    "def count_ngrams(unigrams, bigrams_list, trigrams_list):\n",
    "    unigram_counts = Counter(unigrams)\n",
    "    bigram_counts = Counter(bigrams_list)\n",
    "    trigram_counts = Counter(trigrams_list)\n",
    "    \n",
    "    return unigram_counts, bigram_counts, trigram_counts\n",
    "\n",
    "def report_most_frequent_ngrams(unigram_counts, bigram_counts, trigram_counts):\n",
    "    print(\"Most frequent unigrams:\")\n",
    "    for word, count in unigram_counts.most_common(8):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    print(\"\\nMost frequent bigrams:\")\n",
    "    for bigram, count in bigram_counts.most_common(8):\n",
    "        print(f\"{' '.join(bigram)}: {count}\")\n",
    "\n",
    "    print(\"\\nMost frequent trigrams:\")\n",
    "    for trigram, count in trigram_counts.most_common(8):\n",
    "        print(f\"{' '.join(trigram)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent unigrams:\n",
      "داره: 80\n",
      "گوشی: 72\n",
      "هست: 58\n",
      "کیفیت: 51\n",
      "های: 49\n",
      "استفاده: 41\n",
      "یه: 41\n",
      "میشه: 38\n",
      "\n",
      "Most frequent bigrams:\n",
      "دیجی کالا: 15\n",
      "ممنون دیجی: 11\n",
      "راضی هستم: 10\n",
      "حتما پیشنهاد: 8\n",
      "شگفت انگیز: 7\n",
      "ازش استفاده: 6\n",
      "پیشنهاد ممنون: 6\n",
      "ارزش خرید: 5\n",
      "\n",
      "Most frequent trigrams:\n",
      "حتما پیشنهاد ممنون: 6\n",
      "ممنون دیجی کالا: 5\n",
      "پیشنهاد ممنون دیجی: 5\n",
      "خریدم راضی هستم: 4\n",
      "ارزش خرید داره: 3\n",
      "اصلا پیشنهاد نمی: 3\n",
      "نا امید شدم: 3\n",
      "کیفیت صفحه نمایش: 3\n"
     ]
    }
   ],
   "source": [
    "unigrams_list, bigrams_list, trigrams_list = extract_ngrams(c)\n",
    "uc, bc, tc = count_ngrams(unigrams_list, bigrams_list, trigrams_list)\n",
    "report_most_frequent_ngrams(uc, bc, tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing in N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Smoothing Techniques in N-grams Language Models\n",
    "\n",
    "In n-grams language models, it's common to encounter unseen n-grams, i.e., sequences of words that never occurred in the training data. When calculating the probability of such unseen n-grams, their probability would be zero, which could lead to severe issues in the model's performance, especially during evaluation.\n",
    "\n",
    "To address this problem, smoothing techniques are used. Smoothing assigns a small non-zero probability to unseen n-grams, thereby preventing zero probabilities and making the model more robust. Smoothing techniques distribute the probability mass from observed n-grams to unseen ones in a principled manner.\n",
    "\n",
    "---\n",
    "\n",
    "**Laplace (Add-One) Smoothing**\n",
    "\n",
    "Laplace smoothing, also known as Add-One smoothing, is one of the simplest smoothing techniques. In Laplace smoothing, a count of 1 is added to each observed n-gram count before calculating probabilities. This ensures that no n-gram has zero probability and prevents unseen n-grams from having zero probabilities.\n",
    "\n",
    "Mathematically, the formula for Laplace smoothing of an n-gram is:\n",
    "\n",
    "$ P_{\\text{Laplace}}(w_n | w_{n-1}) = \\frac{{\\text{count}(w_{n-1}w_n) + 1}}{{\\text{count}(w_{n-1}) + V}}$\n",
    "\n",
    "Where:\n",
    "- $\\text{count}(w_{n-1}w_n)$ is the count of the n-gram \\( $w_{n-1}w_n$ \\) in the training data.\n",
    "- $\\text{count}(w_{n-1}) $ is the count of the preceding (n-1)-gram \\( $w_{n-1} $\\) in the training data.\n",
    "- $ V $ is the vocabulary size, representing the total number of unique words in the training data.\n",
    "\n",
    "https://www.nltk.org/api/nltk.probability.LaplaceProbDist.html\n",
    "\n",
    "---\n",
    "\n",
    "**Good-Turing Smoothing**\n",
    "\n",
    "Good-Turing smoothing is a more sophisticated smoothing technique that estimates the probabilities of unseen n-grams based on the observed frequencies of other n-grams. It adjusts the probabilities of unseen n-grams based on the frequencies of seen n-grams with similar frequencies. This technique tends to work well when dealing with sparse data and can provide more accurate estimates than Laplace smoothing.\n",
    "\n",
    "Good-Turing smoothing uses a statistical method called the Good-Turing frequency estimation to estimate the probability of unseen n-grams. It estimates the probability of an unseen n-gram by considering the frequency of n-grams with similar counts in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(unigrams, bigrams, trigrams):\n",
    "    unigram_prob_dist = LaplaceProbDist(nltk.FreqDist(unigrams), bins=len(set(unigrams)))\n",
    "\n",
    "    bigram_prob_dist = SimpleGoodTuringProbDist(nltk.FreqDist(bigrams))\n",
    "    trigram_prob_dist = SimpleGoodTuringProbDist(nltk.FreqDist(trigrams))\n",
    "\n",
    "    return unigram_prob_dist, bigram_prob_dist, trigram_prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_prob_dist, bigram_prob_dist, trigram_prob_dist = calculate_probabilities(unigrams_list, bigrams_list, trigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00026219192448872575 1.2534513957088445e-05 0.000217704306341319\n"
     ]
    }
   ],
   "source": [
    "unigram_prob = unigram_prob_dist.prob('بو')\n",
    "bigram_prob = bigram_prob_dist.prob(('جاداره', 'طراحیش'))\n",
    "trigram_prob = trigram_prob_dist.prob(('ارزش', 'خرید', 'داره'))\n",
    "\n",
    "print(unigram_prob, bigram_prob, trigram_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(prob_dist, ngrams):\n",
    "    likelihoods = [prob_dist.prob(ngram) for ngram in ngrams]\n",
    "    perplexity = np.prod(likelihoods) ** (-1 / len(ngrams))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_log_perplexity(prob_dist, ngrams):\n",
    "    log_likelihood = sum(-np.log(prob_dist.prob(ngram)) for ngram in ngrams)\n",
    "    perplexity = log_likelihood / len(ngrams)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_perplexity_for_models(unigram_prob_dist, bigram_prob_dist, trigram_prob_dist, sentences):\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    perplexities = {'Unigram': [], 'Bigram': [], 'Trigram': []}\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        unigram_perplexity = calculate_log_perplexity(unigram_prob_dist, list(sentence))\n",
    "        bigram_perplexity = calculate_log_perplexity(bigram_prob_dist, list(bigrams(sentence)))\n",
    "        trigram_perplexity = calculate_log_perplexity(trigram_prob_dist, list(trigrams(sentence)))\n",
    "        \n",
    "        perplexities['Unigram'].append(unigram_perplexity)\n",
    "        perplexities['Bigram'].append(bigram_perplexity)\n",
    "        perplexities['Trigram'].append(trigram_perplexity)\n",
    "    \n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Unigram Model:\n",
      "Sentence 1: 7.284726350969215\n",
      "Sentence 2: 7.020115091550757\n",
      "Sentence 3: 6.349809784308699\n",
      "Sentence 4: 6.753836492440416\n",
      "-------------------------------\n",
      "Perplexity for Bigram Model:\n",
      "Sentence 1: 8.065693220532186\n",
      "Sentence 2: 11.287024601969089\n",
      "Sentence 3: 11.287024601969089\n",
      "Sentence 4: 10.729894009691739\n",
      "-------------------------------\n",
      "Perplexity for Trigram Model:\n",
      "Sentence 1: 7.496977197363101\n",
      "Sentence 2: 13.106568675036614\n",
      "Sentence 3: 13.106568675036614\n",
      "Sentence 4: 13.106568675036614\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"این لپ تاپ سخت افزار خیلی قوی داره و از پس هرکاری به راحتی بر میاد\",\n",
    "    \"این ساعت بسیار زیبا طراحی و ساخته شده\",\n",
    "    \"یک محصول با کیفیت ایرانی که حقیقتا جای حمایت داره\",\n",
    "    \"بوش و ماندگاری خوب هست من خیلی دوستش دارم\"\n",
    "]\n",
    "\n",
    "preprocessed_test_sentences = preprocess_sentences(pd.DataFrame({'comment': test_sentences}))\n",
    "perplexities = calculate_perplexity_for_models(unigram_prob_dist, bigram_prob_dist, trigram_prob_dist, preprocessed_test_sentences)\n",
    "\n",
    "for model, perplexity_list in perplexities.items():\n",
    "    print(f\"Perplexity for {model} Model:\")\n",
    "    for sentence_index, perplexity in enumerate(perplexity_list):\n",
    "        print(f\"Sentence {sentence_index + 1}: {perplexity}\")\n",
    "    print('-------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      "کیفیت محصولات چینی زرین نکرد نشدن نمیشه امکانات صورت قطعأ های فدق\n",
      "از لحاظ جنس جنس خوبی داره گوشی بدین شیک یاب باش پراید\n",
      "حتما پیشنهاد میکنم بهتربن روز گوشی کامنت میشه منتظر نداره ۸ کار\n",
      "بعد از چند روز استفاده صفر ارزان آمپر بوی منم ایکاش حساب\n",
      "Bigram:\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Trigram:\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def unigram_predict(sent, freq_dist, max_length=12):\n",
    "    while len(sent.split()) < max_length:\n",
    "        word = freq_dist.generate()\n",
    "        sent = sent + \" \" + word\n",
    "    return sent\n",
    "\n",
    "\n",
    "def bigram_predict(sent, freq_dist, max_length=12):\n",
    "    pass\n",
    "    \n",
    "\n",
    "def trigram_predict(sent, freq_dist, max_length=12):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_sentences = [\n",
    "    \"کیفیت محصولات چینی زرین\",\n",
    "    \"از لحاظ جنس جنس خوبی داره\",\n",
    "    \"حتما پیشنهاد میکنم\",\n",
    "    \"بعد از چند روز استفاده\"\n",
    "]\n",
    "\n",
    "print('Unigram:')\n",
    "for sent in test_sentences:\n",
    "    sentence = unigram_predict(sent, unigram_prob_dist)\n",
    "    print(sentence)\n",
    "\n",
    "print('Bigram:')\n",
    "for sent in test_sentences:\n",
    "    sentence = bigram_predict(sent, bigram_prob_dist)\n",
    "    print(sentence)\n",
    "\n",
    "print('Trigram:')\n",
    "for sent in test_sentences:\n",
    "    sentence = trigram_predict(sent, trigram_prob_dist)\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
